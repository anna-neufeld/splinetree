---
title: "Introduction to Spline Trees"
author: "Brianna Heggeseth and Anna Neufeld"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This paper is meant to serve as a thorough introduction to the ``splinetree`` package, including the motivation and the details of the splitting algorithm. For a more concise and practical guide to tree building or forest building with spline tree, see the other Vignettes- *Introduction to Tree Building with splinetree* and *Introduction to Forest Building with splinetree*.  

# Motivation

Longitudinal studies, where an outcome of interest is measured in the same subjects over time, play a key role in research across a variety of disciplines, from medicine and epidemiology to the social and behavioral sciences. In these studies, it is often useful to understand whether different groups of the population display distinct trajectory patterns. This can be done through clustering the trajectories of different observational units into homogeneous groups. We propose using longitudinal regression trees, specifically ``splinetree``, to uncover subgroups of homogeneous trajectories where each subgroup is labeled by common covariate values. Then, to obtain more stable measures of variable importance, we propose using longitudinal random forests. 

Longitudinal trajectories have two prominent features: level and shape of change over time. The ``splinetree`` package makes it possible to form clusters based solely on shape while ignoring the level of the trajectory. The splitting methodology used in the ``splinetree'' package is based off the work of Yu and Lambert, 1999. Yu and Lambert suggest treating time series data as a functional curve and reducing the dimension of the outcome vector using smoothing splines.

$$Y_i(t) = f_i(t) + \epsilon_i(t)$$

where $f_i(t) = \sum_{k = 1}^q\beta_{ik} X_k(t)$ for a set of basis functions, $X_k(t)$, and coefficient vector $\boldsymbol{\beta}_i = (\beta_{i1},...,\beta_{iq})^T$, and where $\epsilon_i(t)$ is white noise with mean zero and constant variance. A regression tree is then built using the coefficient vectors $\boldsymbol{\beta}_i = (\beta_{i1},...,\beta_{iq})^T$ as the response. 

The split criteria for the tree building process uses a projection sum of squares. At a specified set of points, the sum of squared errors between each individual projected trajectory and the mean projected trajectory is calculated. The goal of the splitting process is to divide the data into subgroups where the projection sum of squares is minimized. 

This framework allows for the modeling of complex, non-linear trajectories with internal knots. By ignoring the coefficient corresponding to the intercept in the splitting process, the splitting methodology can take into account only change over time while ignoring level. Once the dimesnion of the response is reduced to a set of coefficient vectors, the tree-splitting process essentially follows CART (Brieman, 1984) with a modified split function, which means thatit can be implemented using the custom function functionality of the widely used package rpart. 

This vignette is intended as a straightforward guide to using our package to create longitudinal regression trees and longitudinal random forests using the ``splinetree'' package. 

# Building and Understanding a Spline Tree

In this section, we will walk through the process of building a tree, from choosing a dataset to visualizing and evaluating the model. For our examples, we will use data from the National Longitudinal Survey of Youth, 1979 (NLSY). Specifically, we choose body mass index (BMI) as our response variable, and numerous variables related to baseline soioeconomic status and family background as our covariates. We randomly sample 1,000 individuals from the NLSY who have non-missing BMI data at at least 10 timepoints spread out over at least 20 years. 

## Preparing and Understanding the Data

To begin, the longitudinal dataset should be in long format. This means that for each observational unit (in this example, each person) there are many different rows of data corresponding to individual measurements taken at different values of the "time" variable (in our example, at different ages). Different rows of data that belong to the same individual are linked together by a common ID variable- the name of this variable will be passed to the ``splineTree()`` function as ``idvar``. 
We next need to specify the variables that define our trajectory of interest for our tree. In our example, our longitudinal response of interest is body mass index (BMI). We are interested in trajectories of BMI over age, and so the ``tformula`` parameter of the ``splineTree()`` function will be ``BMI~AGE``. 

After defining the trajectory of interest, be must select split variables. Split variables are candidates for variables that may explain some heterogeneity in trajectories. In our case, these are the family background socioeconomic variables collected by the NLSY. 

At this time, the ``splinetree`` package does not support time-varying split covariates. This means that the variables of interest other than the response and the "time" variable should be constant for a given individual. So, if there are 10 rows of data corresponding to the person with ID 372 and you wish to use ``X`` as a split variable, all 10 rows should hold identical values of ``X``. For example, in the NLSY data, we can use ``HGC_MOTHER`` and ``HGC_FATHER`` (highest grade completed by an individual's mother and father, respectively) as split variables, because these variables (as recorded by the NLSY, at least) do not change for a given individual as they age. However, we cannot use ``HGC`` (highest grade completed by the subject) as a split variable because, as subjects age and attend more school, this value changes. 

```{r, include=FALSE}
library(splinetree)
```

```{echo=FALSE}
head(nlsySample[,c(1,26,31, 12,14,27)], n=20)
```

If we were interested in including time-varying covariates as split variables because we believed that they were related to heterogeneity in trajectory, we could summarize the variable in some way and use this summary as a new, time-constant split variable. For example, we could define a new variable, HGC_MAX, that would record the maximum value of HGC for each individual. This would allow us to associate a single BMI trajectory with the highest grade that an individual eventually completes. In other cases, a time-varying covariate could be summarized with an average, or the the time-varying covariate could be regressed against time and the slope coefficient could be used as the new covariate. In our example, we will not do this at all, and will instead use time-constant split variables.

## Choosing Tree Parameters 

### Specifying a Spline Basis

After identifying the variables of interest in the dataset, we must specify the spline basis that we will project each individual's responses onto. Specifying a spline basis requires specifying the degree and the number/location of internal knots. In picking a degree for the spline basis, it is helpful to know something about the trajectories that you are modeling. For example, previous research has found that, on average, adult BMIs tend to increase steadily throughout early adulthood and then flatten out in later adulthood (Clarke et al. 2008). This type of trajectory could be modeled either with a quadratic trajectory or a linear trajectory with an internal knot, and so picking ``degree=1`` or ``degree=2`` for our example seems reasonable. It is also reasonable to build several different trees with different degrees and comparing and contrasting. 

The boundary knots for the spline basis are always placed automatically at the minimum and maximum values of the time variable in the dataset. By default, the package will not include any internal knots in the spline basis. If we wish to add an internal knot but we do not have a specific location in mind for the knot, we can do this by increasing the ``df`` parameter (degrees of freedom). The  degrees of freedom specifies the total number of spline coefficients that will be used in the tree, and always equals degree + number of internal knots + intercept. So, if we are building a linear tree without an intercept, specifying ``df=3`` will have the effect of adding two internal knots. These internal knots will be placed at the appropriate percentiles of the time variable (in this case the 33rd and 66th percentile). If on the other hand we are building a cubic tree with an intercept and we want one automatically placed internal knot, we would set ``df=6``, and the knot would be placed at the 50th percentile of the time variable. 

If we have some specific location in mind for an internal knot, we can provide this with the ``knots`` parameter. For example, suppose that previous research told us that BMI trajectories tend to change dramatically at age 35. If age 35 does not happen to be the 50th percentile of all the ages in the training data, the ``df`` parameter will not help us. The knots parameter allows us to pass in a vector of specific locations for knots. Note that providing a specific list of internal knots (along with providing the degree and whether or not to use an intercept) uniquely specifies the degrees of freedom of the basis. Therefore, in calling splineTree() you should never provide both ``df`` and ``knots``. 

While it may seem tempting to use a quartic spline with 10 internal knots to capture very precise summaries of the true trajectories, doing so will likely cause problems. If the degrees of freedom exceeds the number of observations for an individual in the dataset, then this individual will have `NA` values for some of their spline coefficients and will be thrown out during the tree building process. With 10 degrees of freedom, all individuals with less than 10 measurements will be thrown out, which may reduce the size of the dataset significantly. If some individual in the dataset has more than 10 observations, but all of these observations happen to be bunched up to one side of an internal knot, this individual will not be thrown out from the tree, but they may be assigned very large, unpredictable coefficients that will disrupt the tre building process. This problem that arises is another reason to avoid adding more internal knots than necessary. It is also a good idea to make sure before beginning that most individuals in the dataset are measured over a similar range of the time variable.

### Specifying the Projection Grid

Once the spline basis has been chosen, we must specify how we want the projection sum-of-squares to be calcualted. 

The first important choice is if we want the projection sum of squares to ignore or include the intercept coefficients of the projected trajectories. If the parameter ``intercept`` is set to TRUE, the splitting process takes into account both level and shape of response. If the paramter `intercept`` is set to ``FALSE``, then when computing the projection sum of squares the split function will ignore the coefficients corresponding to the intercept of the trajectories. Therefore, the 
splitting process takes into account only the shape of the trajectory while ignoring the level of the response. If the intercept is excluded, the splits of the tree will inform us about which variables impact the shape of a trajectory, but the downside is that the terminal nodes of the tree cannot be used for prediction.

Finally, we must choose the grid that we want our projection sum of squared errors to be evaluated on. This can be done either through supplying an integer to ``nGrid``, which specifies the number of gridpoints to be placed automatically at quantiles of the time variable, or through ``gridPoints,`` which is a vector of numbers that define the grid points themselves. Only one of these parameters, not both, should be provided. If ``gridPoints`` is provided, the values in the vector should fall roughly in the range of the time variable in the dataset. 

### Controlling Tree Size

The final parameters are ``cp`` and ``minNodeSize``. These are passed forward to ``rpart`` for the actual tree-building 

## Fitting the model

After that long discussion of parameters, we are ready to build a model. 


We pretty much learned what all the parameters were above. Suppose we cannot decide if we would rather use a linear basis with an internal knot or a quadratic basis. Either might be reasonable based on previous research. 
- Example code of building a spline tree. Show some stuff about how to print a summary. 
- Explain that the model is just an rpart object which is nice. Has extra info stored in parms in case you forget.
- Show a few with different types of coefficients. 

## Plotting the model

- Example code of plotting a spline tree

## Evaluating the model

- R2-type measures. Explain difference between them 


# Building and Understanding a Spline Forest
- If you don't care so much about interpretability, but you really care about variable importance, you might want to build a forest. 

## Building the forest
- Warning- this code runs very slowly. We highly recommend saving your forest as .RData when it runs so that later you do not have to rebuild it
- 

## Evaluating the forest
- 

## Variable Importance
- You CAN do variable importance from a single tree (show this)
- You can also do variable importance from a forest
- Compare the two. 
- Show shortcomings of the single tree version (example- dependent on pruning,e tc. )

